# Отчет по первой лабораторной работе

## 1. Теоретическая база

В основе данной работы лежит задача классификации изображений с применением методов глубокого обучения. Современные нейронные сети, обучаемые на больших наборах данных, демонстрируют высокую точность в решении таких задач. Особое место в этой области занимают сверточные нейронные сети (CNN), которые используют операции свёртки для выделения признаков из изображений и уже многие годы остаются одним из основных инструментов компьютерного зрения.

Идея CNN заключается в том, чтобы применить набор фильтров ко входному изображению, постепенно выявляя как низкоуровневые признаки (например, контуры или грани), так и более сложные структуры, формируемые на последующих слоях. В отличие от классических подходов, где специалистам приходилось вручную выделять особенности изображений, глубокие сети обучаются делать это автоматически. Одной из первых моделей, продемонстрировавших значительное улучшение качества в задачах классификации, стала сеть AlexNet. Она была предложена Алексом Кризевским и соавторами и вошла в историю машинного обучения благодаря победе в конкурсе ImageNet в 2012 году. AlexNet отличается относительно глубокой по тем временам архитектурой, использованием функции активации ReLU, а также механизмом дропаута для снижения переобучения.

Большое влияние на точность и скорость обучения оказывают методы оптимизации. В основе обучения нейросетей лежит процедура минимизации функции потерь с помощью градиентных методов. Однако простые варианты, такие как обычный стохастический градиентный спуск, оказались недостаточно гибкими для эффективной работы с крупными моделями и большими датасетами. Поэтому были разработаны адаптивные оптимизаторы, например Adam, учитывающий моменты первого и второго порядка градиентов для динамической настройки скоростей обучения различных параметров. В исследуемой работе добавлен ещё один оптимизатор — AdaSmooth, который представляет собой вариацию адаптивного подхода с дополнительным сглаживанием для более стабильной сходимости. 

В качестве набора данных для экспериментов был выбран Stanford Cars, содержащий большое число изображений автомобилей различных марок и моделей. Задача состоит в том, чтобы правильно классифицировать каждое изображение по принадлежности к одному из заданных классов.

## 2. Описание разработанной системы

**Принцип работы системы:**
- Система использует датасет Stanford Cars, который состоит из изображений автомобилей и аннотаций, описывающих класс (марку и модель).
- Датасет разбит на тренировочную и тестовую выборки.
- Изображения нормализуются и масштабируются под размер, ожидаемый входом AlexNet (224x224).
- AlexNet инициализируется с числом выходных нейронов, соответствующим количеству классов.
- Запускается процесс обучения: сеть итеративно обрабатывает батчи изображений, вычисляет ошибку (кросс-энтропийная функция потерь) и обновляет веса с помощью выбранного оптимизатора.
- После обучения модель оценивается на тестовой выборке, вычисляется точность классификации.

**Архитектура:**
- **Входные данные:** изображения формата JPEG.
- **Предобработка:** изменение размера, нормализация.
- **Модель:** AlexNet (Conv + ReLU + MaxPool слои, затем полносвязные слои для классификации).
- **Функция потерь:** Кросс-энтропия.
- **Оптимизаторы:** Adam, AdaSmooth.
- **Инфраструктура:** обучается в среде PyTorch, с использованием GPU.
  
Алгоритмически:
1. Загрузка и парсинг аннотаций из .mat файлов.
2. Создание Dataset и DataLoader для удобной итерации по данным.
3. Определение и инициализация модели.
4. Выбор и инициализация оптимизаторов Adam и AdaSmooth.
5. Запуск циклов обучения.
6. Сравнительный анализ результатов после каждой эпохи.

## 3. Результаты работы и тестирования системы

Ниже приведены примеры результатов:

**Пример выводов в процессе обучения (консоль):**
```
Эпоха 1, Потери: 5.2786, Точность: 0.48%
Эпоха 2, Потери: 5.2775, Точность: 0.82%
Эпоха 3, Потери: 5.2376, Точность: 1.08%
Эпоха 4, Потери: 5.1693, Точность: 1.26%
Эпоха 5, Потери: 5.1361, Точность: 1.36%
Эпоха 6, Потери: 5.1094, Точность: 1.61%
Эпоха 7, Потери: 5.0572, Точность: 1.81%
Эпоха 8, Потери: 5.0036, Точность: 2.19%
Эпоха 9, Потери: 4.9579, Точность: 2.81%
Эпоха 10, Потери: 4.9150, Точность: 2.87%
Тестовая точность: 2.50%
```

## 4. Выводы по работе

- Использование сверточной нейронной сети AlexNet позволило успешно решить задачу классификации изображений автомобилей по классам.
- Нормализация изображений и применение архитектуры CNN существенно повышает точность по сравнению с классическими методами компьютерного зрения.
- Оптимизаторы Adam и AdaSmooth демонстрируют схожую динамику обучения, однако в некоторых экспериментах AdaSmooth показывает более плавную сходимость и чуть более высокую итоговую точность.
- Данный подход может быть расширен для использования более современных архитектур (ResNet, EfficientNet) и других адаптивных оптимизаторов.

## 5. Использованные источники

Крижевский А. В., Суцкевер И. В., Хинтон Г. Е. Классификация ImageNet с глубокими сверточными нейронными сетями // Коммуникации АКМ. – 2017. – Т. 60, No6. – С. 84–90.

Кингма Д., Ба Дж. Адам: метод стохастической оптимизации [электронный ресурс] // Международная конференция по обучающимся представлениям (ICLR). – 2015. – Режим доступа: https://arxiv.org/abs/1412.6980 (дата обращения: 17.12.2024).

Stanford Cars Dataset [электронный ресурс]. – Режим доступа: http://ai.stanford.edu/~jkrause/cars/car_dataset.html (дата обращения: 17.12.2024).

Документация PyTorch [Электронный ресурс]. – Режим доступа: https://pytorch.org/docs/stable/ (дата обращения: 17.12.2024).

Рудер С. Обзор алгоритмов оптимизации градиентного спуска [Электронный ресурс]. – 2016. – Режим доступа: https://arxiv.org/abs/1609.04747 (дата обращения: 17.12.2024).
